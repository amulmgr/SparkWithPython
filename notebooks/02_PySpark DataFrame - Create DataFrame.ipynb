{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Create Dataframes\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataframe with list of tupes and explicit schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "data = [\n",
    "    (1, 1., 'abc', date(2020, 1, 1), datetime(2020, 1, 1, 12, 0)),\n",
    "    (2, 2., 'xyz', date(2020, 2, 1), datetime(2020, 1, 2, 12, 0)),\n",
    "    (3, 3., 'a12', date(2020, 3, 1), datetime(2020, 1, 3, 12, 0))\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    data, schema='col1 long, col2 double, col3 string, col4 date, col5 timestamp'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col1: long (nullable = true)\n",
      " |-- col2: double (nullable = true)\n",
      " |-- col3: string (nullable = true)\n",
      " |-- col4: date (nullable = true)\n",
      " |-- col5: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-------+----------+-------------------+\n",
      "|col1|col2|   col3|      col4|               col5|\n",
      "+----+----+-------+----------+-------------------+\n",
      "|   1| 2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|   2| 3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|   3| 4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+----+----+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataframe with list of Rows\n",
    "\n",
    "pyspark.sql.Row : A row of data in a DataFrame.\n",
    "\n",
    "- Row can be used to create a row object by using named arguments, the fields will be sorted by names. \n",
    "- It is not allowed to omit a named argument to represent the value is None or missing. This should be explicitly set to None in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(col1=1, col2=1., col3='abc', col4=date(2020, 1, 1), col5=datetime(2020, 1, 1, 12, 0)),\n",
    "    Row(col1=2, col2=2., col3='xyz', col4=date(2020, 2, 1), col5=datetime(2020, 1, 2, 12, 0)),\n",
    "    Row(col1=3, col2=3., col3='a12', col4=date(2020, 3, 1), col5=datetime(2020, 1, 3, 12, 0))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col1: long (nullable = true)\n",
      " |-- col2: double (nullable = true)\n",
      " |-- col3: string (nullable = true)\n",
      " |-- col4: date (nullable = true)\n",
      " |-- col5: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----------+-------------------+\n",
      "|col1|col2|col3|col4      |col5               |\n",
      "+----+----+----+----------+-------------------+\n",
      "|1   |1.0 |abc |2020-01-01|2020-01-01 12:00:00|\n",
      "|2   |2.0 |xyz |2020-02-01|2020-01-02 12:00:00|\n",
      "|3   |3.0 |a12 |2020-03-01|2020-01-03 12:00:00|\n",
      "+----+----+----+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataFrame from a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-d48db3dde9a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m pd_df = pd.DataFrame({\n\u001b[0;32m      4\u001b[0m     \u001b[1;34m'col1'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;34m'col2'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd_df = pd.DataFrame({\n",
    "    'col1': [1, 2, 3],\n",
    "    'col2': [1., 2., 4.],\n",
    "    'col3': ['abc', 'xyz', 'a12'],\n",
    "    'col4': [date(2020, 1, 1), date(2020, 2, 1), date(2020, 3, 1)],\n",
    "    'col5': [datetime(2020, 1, 1, 12, 0), datetime(2020, 1, 2, 12, 0), datetime(2020, 1, 3, 12, 0)]\n",
    "})\n",
    "df = spark.createDataFrame(pd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\satee\\anaconda3\\envs\\pyspark\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Collecting pytz>=2017.2\n",
      "  Downloading pytz-2021.1-py2.py3-none-any.whl (510 kB)\n",
      "Collecting numpy>=1.15.4\n",
      "  Downloading numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\satee\\anaconda3\\envs\\pyspark\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Installing collected packages: pytz, numpy, pandas\n",
      "Successfully installed numpy-1.19.5 pandas-1.1.5 pytz-2021.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd_df = pd.DataFrame({\n",
    "    'col1': [1, 2, 3],\n",
    "    'col2': [1., 2., 4.],\n",
    "    'col3': ['abc', 'xyz', 'a12'],\n",
    "    'col4': [date(2020, 1, 1), date(2020, 2, 1), date(2020, 3, 1)],\n",
    "    'col5': [datetime(2020, 1, 1, 12, 0), datetime(2020, 1, 2, 12, 0), datetime(2020, 1, 3, 12, 0)]\n",
    "})\n",
    "df3 = spark.createDataFrame(pd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col1: long (nullable = true)\n",
      " |-- col2: double (nullable = true)\n",
      " |-- col3: string (nullable = true)\n",
      " |-- col4: date (nullable = true)\n",
      " |-- col5: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----------+-------------------+\n",
      "|col1|col2|col3|col4      |col5               |\n",
      "+----+----+----+----------+-------------------+\n",
      "|1   |1.0 |abc |2020-01-01|2020-01-01 12:00:00|\n",
      "|2   |2.0 |xyz |2020-02-01|2020-01-02 12:00:00|\n",
      "|3   |4.0 |a12 |2020-03-01|2020-01-03 12:00:00|\n",
      "+----+----+----+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
